---
title: "LandSat 8 Modelling"
author: "Jody Holland"
format: html
editor: visual
bibliography: lsmodelling.bib
---

# Introduction

The first methodological step is to train a variety of models predicting LST using LandSat 8 at 30m resolution. This is in a similar process to @onacillová2022; @equere2021; @son2017; @karyati2022. In this modelling process we employ two families of modelling approaches:

-   Linear Regression [@onacillová2022]

-   Neural Networks [@equere2021]

Through comparing the accuracy of these models we make inferences on which is most likely to provide high utility data for use in modelling 10m resolution LST with Sentinel 2 data as an input.

## Libraries

This methodology will use the following r packages for data wrangling using data from July 24th 2023 (a day when both Sentinel and Landsat imagery coincides).

-   The tidyverse family of packages [@tidyverse], which includes

    -   dplyr for wrangling data [@dplyr]

    -   tidyr for managing data sets [@tidyr]

    -   ggplot2 for visualisations [@ggplot2]

-   The terra package for handling raster data [@terra-2]

-   The RColorBrewer package for colour scales [@RColorBrewer]

-   The sf package for handling shapefile data [@sf]

-   The leaflet package for interactive visualisation [@leaflet]

-   The osmdata package for extracting coastline data [@rnaturalearth]

-   The modelsummary package for outputting regression results [@modelsummary]

-   The ggnewscale package for help with layered plots [@ggnewscale]

```{r, message=FALSE}
# load packages
library(tidyverse)
library(terra)
library(RColorBrewer)
library(sf)
library(leaflet)
library(osmdata)
library(modelsummary)
library(ggnewscale)
library(nnet)
```

# Preparing Data

To train these models, we use input data from Landsat covering and clipped to our study area of Maspalomas-Playa del Inglés. This raster data is broken down into broken down into bands from which the following can be calculated:

-   LST

-   NDVI (normalised difference vegetation index)

-   NDBI (normalised difference built-up index)

-   NDWI (normalised difference water index)

-   NDSI (normalised difference sand index)

-   Surface Albedo

We will also use the following auxiliary data

-   Road Proximity (from OSM)

-   Proximity to Coast/Ocean

-   Elevation

## Loading Rasters

These rasts denote different bands of EM radiation captured by the LandSat 8 satellite over the south of Gran Canaria at 11:29am on the 24th July 2023 [@u.s.geologicalsurey2023]. These bands cover most of the visible light spectrum as well as the infrared spectrum. Combined in various was they can be used to calculate various measures such as the NDVI (normalised difference vegetation index) and the LST (land surface temperature).

First lets plot and load the rasts

```{r}
# function to plot raster with a label and custome colour scale
plot_rast = function(raster, label, colors) {
  terra::plot(raster, col=colorRampPalette(colors)(100))
  mtext(text=label, side=3, line=2)
}

# define the bounding box epsg:4083
bbox = ext(c(437292.6282,445376.8775,3067829.6809,3072637.1023))

# load each raster, crop to bounding box, and plot with label
b1 = crop(rast("landsat/B1.TIF"), bbox)
b2 = crop(rast("landsat/B2.TIF"), bbox)
b3 = crop(rast("landsat/B3.TIF"), bbox)
b4 = crop(rast("landsat/B4.TIF"), bbox)
b5 = crop(rast("landsat/B5.TIF"), bbox)
b6 = crop(rast("landsat/B6.TIF"), bbox)
b7 = crop(rast("landsat/B7.TIF"), bbox)
b10 = crop(rast("landsat/B10.TIF"), bbox)

# load elevation raster, resample, and crop
strm = rast("strm/strm.tif") %>% 
  project(crs(b1)) %>%
  resample(b1, method = "bilinear") %>%
  crop(bbox)
```

## Loading Coast Shapefile

Using OSM Data to first extract a coastline of the island, this process enables such analysis as calculating distance from the coastline

```{r, warning=FALSE}
# query osm
gran_canaria_query = opq(bbox = "Gran Canaria") %>%
  add_osm_feature(key = "place", value = "island") %>%
  osmdata_sf()
# extract the coast data
gran_canaria_sf = gran_canaria_query$osm_multipolygons

# make spatial vector
gran_canaria_vect = st_transform(gran_canaria_sf, crs(b1)) %>%
  vect()
```

## Loading Road Shapefile

Again using OSM data, it is possible to extract road data in the form of a spatial vector.

```{r}
# define bbox
bbox_osm = c(-15.636205673397361,
             27.73337624747081,
             -15.554409027542757,
             27.777126576546703)

# query osm
osm_roads = opq(bbox = bbox_osm) %>%
  add_osm_feature(key = "highway") %>%
  osmdata_sf()

# extract road lines
roads_vect = st_transform(osm_roads$osm_lines, crs(b1)) %>%
  vect()
```

## Loading Tourism Shapefile

```{r}
# query osm
osm_tourism = opq(bbox = bbox_osm) %>%
  add_osm_feature(key = "tourism") %>%
  osmdata_sf()

# extract tourism points
tourism_vect = st_transform(osm_tourism$osm_points, crs(b1)) %>%
  vect()

plot(tourism_vect)
```

## Loading Building Shapefile

```{r}
# query osm
osm_building = opq(bbox = bbox_osm) %>%
  add_osm_feature(key = "building") %>%
  osmdata_sf()

# extract building polygons
building_vect = st_transform(osm_building$osm_polygons, crs(b1)) %>%
  vect()

plot(building_vect)
```

## Masking Ocean Values

```{r}
b1 = mask(b1, gran_canaria_vect)
b2 = mask(b2, gran_canaria_vect)
b3 = mask(b3, gran_canaria_vect)
b4 = mask(b4, gran_canaria_vect)
b5 = mask(b5, gran_canaria_vect)
b6 = mask(b6, gran_canaria_vect)
b7 = mask(b7, gran_canaria_vect)
b10 = mask(b10, gran_canaria_vect)
strm = mask(strm, gran_canaria_vect)
```

## Calculating NDVI

Perhaps the most important metric beside LST is NVDI. NDVI is also used in the computation of impassivity and ultimately therefore emissivity adjusted LST.

To calculate NDVI, the following formula is used:

$$
\text{NDVI} = \frac{\text{NIR} - \text{Red}}{\text{NIR} + \text{Red}}
$$

Here $NIR$ represents the Near-Infrared band reflectance value, in Landsat 8 this is Band 5. Further $Red$ represents the Red band reflectance value, in Landsat 8 this is Band 4.

```{r, warning=FALSE}
# calc and plot ndvi using bands 5 and 4
ndvi = (b5-b4)/(b5+b4)

plot_rast(ndvi,
          "Normalised Difference Vegetation Index (NDVI)",
          c("brown", "green"))
```

## Calculating NDBI

Another important metric is NDBI as urban artificial surfaces may trap heat causing excess heating relative to vegetated surfaces.

To calculate NDBI for Landsat 8, the following formula is used [@abulibdeh2021]:

$$
\text{NDVI} = \frac{\text{MIR} - \text{NIR}}{\text{MIR} + \text{NIR}}
$$

Here $NIR$ again represents the Middle-Infrared band value, in Landsat 8 this is Band 5. Also $NIR$ again represents the Near-Infrared band value, being Landsat 8 Band 5.

However, we may not use this as for arid areas it has reduced efficacy. Instead we may rasterise OSM building data.

```{r}
ndbi = (b6-b5)/(b6+b5)

plot_rast(ndbi,
          "Normalised Difference Built Index (NDBI)",
          c("lightgreen", "black"))
```

## Calculating NDWI

Complementing these metrics is NDWI, here used to analyse the impact that surface water has upon the temperature.

To calculate NWBI for Landsat 8, the following formula is used:

$$
\text{NDVI} = \frac{\text{Green} - \text{SWIR}}{\text{Green} + \text{SWIR}}
$$

Here $Green$ represents the Green reflectance values, in Landsat 8 this is Band 3. Also $SWIR$ again represents the Short-Infrared band value, being Landsat 8 Band 7.

```{r}
# calc and plot
ndwi = (b3-b7)/(b3+b7)

plot_rast(ndwi,
          "Normalised Difference Water Index (NDWI)",
          c("yellow", "blue"))
```

## Calculating Surface Albedo

There is also surface albedo $\alpha$, which is the the fraction of the reflected shortwave radiation, is calculated using the following formula [@equere2021]:

$$
\alpha = \frac{0.356·\text{Blue} + 0.0130 · \text{Red} + 0.373 · \text{NIR} + 0.085 * \text{MIR} + 0.072 · \text{SWIR} - 0.0018}{1.016}
$$

Here $Blue$ represents the Blue reflectance values, in Landsat 8 this is Band 2. $Red$ again represents the Red band value, being Landsat 8 Band 4. $NIR$ represents the Near-Infrared values, in Landsat 8 this is Band 5. $MIR$ likewise represents the Middle-Infrared values, being Landsat 8 Band 6. Finally $SWIR$ represents the Near-Infrared values, in Landsat 8 this is Band 7.

```{r}

# calc and plot
albedo = (0.356*b2+0.0130*b4+0.373*b5+0.085*b6+0.072*b7-0.0018)/1.016

plot_rast(albedo,
          expression(paste("Surface Albedo (", alpha,")")),
          c("pink", "blue"))
```

## Calculating Distance from Ocean

In terms of other environmental variables, there is also distance from the coast to consider, calculated using OSM coastline data.

```{r}
# create blank template
template = crop(rast("landsat/B1.TIF"), bbox)

# crop sp by bbox
gran_canaria_vect = crop(gran_canaria_vect, bbox)

# create land raster
sea_mask = rasterize(gran_canaria_vect,
                      template,
                      NA,
                      background=1)


# calc distance and plot
coastdistance = distance(sea_mask) %>%
  mask(gran_canaria_vect) / 1000
terra::plot(coastdistance, col = hcl.colors(100))
plot(gran_canaria_vect, add = TRUE)
mtext(text="Coastal Distance (km)", side=3, line=2)
```

## Calculating Road Heatmap

Furthermore, there is distance from the nearest road, again calculated using OSM data.

```{r}
# create road raster
road_mask = rasterize(roads_vect,
                      template,
                      1,
                      background=NA)

road_kernal = focalMat(road_mask, 100, "Gauss")
road_heat = focal(road_mask, road_kernal,
                  fun = sum, na.rm = TRUE)
road_heat[is.na(road_heat)] = 0
road_heat = road_heat %>%
  mask(gran_canaria_vect) %>%
  scale()

plot(road_heat)
plot(gran_canaria_vect, add = TRUE)
plot(building_vect, add = TRUE)
mtext(text="Road Exposure", side=3, line=2)
```

## Calculating Tourism Exposure

Furthermore, there the effect of exposure to tourism, modelled here using a heatmap, again calculated using OSM data.

```{r}
# create tourism raster
tourism_mask = rasterize(tourism_vect,
                         template,
                         1,
                         background=NA)

# create heatmap
tourism_kernal = focalMat(tourism_mask, 200, "Gauss")
tourism_heat = focal(tourism_mask, tourism_kernal, 
                     fun = sum, na.rm = TRUE) 
tourism_heat[is.na(tourism_heat)] = 0
tourism_heat = tourism_heat %>%
  mask(gran_canaria_vect) %>%
  scale()

# plot
plot(tourism_heat)
plot(gran_canaria_vect, add = TRUE)
plot(building_vect, add = TRUE)
mtext(text="Tourism Exposure", side=3, line=2)
```

## Calculating Building Footprint Heatmap

```{r}
# create buidling raster
building_mask = rasterize(building_vect,
                          template,
                          1,
                          background=NA)

# create heatmap
building_kernal = focalMat(building_mask, 199, "Gauss")
building_heat = focal(building_mask, building_kernal,
                      fun = sum, na.rm = TRUE) 
building_heat[is.na(building_heat)] = 0
building_heat = building_heat %>%
  mask(gran_canaria_vect) %>%
  scale()

plot(building_heat)
plot(gran_canaria_vect, add = TRUE)
plot(building_vect, add = TRUE)
mtext(text="Building Concentration", side=3, line=2)
```

## Calculating Elevation

As elevation is already loaded in 30m resolution the STRM project. The processing steps here are just to plot and visualise it.

```{r}
# plot
plot_rast(strm,
          "Elevation (m)",
          c("green", "brown"))
```

## Calculating LST

Calculating LST can be straightforward in principle, using Band 10 alone of Landsat 8 data. However, this will not account for surface emissivity. Thus, for this study, an adjusted LST is calculated.

### Top of Atmosphere Spectral Radiance

The first step is to convert the Digital Numbers $DN$ in the Band 10 layer to Top of Atmosphere Spectral Radiance $L_\lambda$ values. This process uses input units found the images metadata/MLT file. These values are the RADIANCE_MULT_BAND_10 $M_L$ and the RADIANCE_ADD_BAND_10 $\text{A}_L$.

Putting these together into a formula is as follows:

$$
\text{L}_\lambda = \text{M}_L · \text{DN} + \text{A}_L
$$

```{r}
# define metadata values
mult_b10 = 3.8e-04
add_b10 = 0.1

# calc and plot
l = (mult_b10*b10+add_b10)

plot_rast(l,
          expression(Spectral~Radiance~(L[lambda])),
          c("brown", "yellow"))
```

### At-Sensor Temperature

The second step is "Conversion of Radiance to At-Sensor Temperature" [@karyati2022]. This This converts the Spectral Radiance into temperature values or Brightness Temperature $BT$ in Kelvin. This process requires two constants related to Landsat 8 Band 10's specific thermal conversion constants found again in the metadata file. These are $K1$and $K2$.

Putting this together into a formula is as follows:

$$
\text{BT} = \frac{\text{K2}}{ln(\frac{\text{K1}}{\text{L}_\lambda} + 1)} 
$$

```{r}
# add effective wavelength of landsat b10
k1 = 799.0284
k2 = 1329.2405

# calc and plot
bt = (k2/log(k1/l+1))

plot_rast(bt,
          expression(Brightness~Temp~(BT)),
          c("blue", "red")) 
```

### Surface Level Emissivity

The above metric alone could be used for study of heat distribution, however it does not account for the variation caused by surface level emissivity. Emissivity denotes how much infrared a surface will absorb or reflect, ranging from a mirror to a perfect black surface. The values of emissivity range from 0 to 1 respectively.

#### Fractional Vegetation Factor

NDVI is used to determine emissivity, through the fractional vegetation factor $\text{P}_v$. This value is specific to the region being studied.

The formula for calculating fractional vegetation factor is as follows:

$$
\text{P}_v = \left( \frac{\text{NDVI} - \text{NDVI}_{\min}}{\text{NDVI}_{\max} - \text{NDVI}_{\min}} \right)^2
$$

```{r}
# define min and max ndvi
min_ndvi = global(ndvi, fun=min, na.rm=TRUE) %>%
  as.numeric()
max_ndvi = global(ndvi, fun=max, na.rm=TRUE) %>%
  as.numeric()

# calc and plot vegetation factor
pv = ((ndvi-min_ndvi)/(max_ndvi-min_ndvi))^2

plot_rast(pv,
          expression(Fractional~Vegetation~Factor~(P[v])),
          c("maroon", "green"))
```

#### Emissivity

From this the surface emissivity can be calculated. This uses two constants relating to the relative emissivity of soil $\epsilon\text{s}\lambda$ and the emissivity of vegetation $\epsilon\text{v}\lambda$, as well as one surface roughness $\text{C}\lambda$ metric of 0.005

The formula for estimating emissivity $\epsilon\lambda$ is thus as follows:

$$
\epsilon\lambda = \epsilon\text{v}\lambda + \epsilon\text{s}\lambda  · (1 - \text{P}_v) + \text{C}\lambda
$$

```{r}
# soil emissivity
es = 0.964

# veg emissivity
ev = 0.984

# calc and plot emissivity
e =  ev*pv+es*(1-pv)+0.005

plot_rast(e,
          expression(Emissivity~(epsilon*lambda)),
          c("yellow", "darkgreen"))
```

### Correction Constant

Now most of the building blocks are in place to calculate the adjusted LST. This requires a correction constant $\rho$ that incorporates the Speed of Light $c$, Planck's constant $h$, and Boltzmann's constant $\sigma$.

The formula for this is as follows:

$$
\rho = \frac{c·h}{\sigma}
$$

```{r}
# speed of light
c = 2.997925e8 

# planck's constant
h = 6.626070e-34

# boltzmann's constant
sigma = 1.380649e-23

# calc and output correction constant/rho 
rho = c*h/sigma
rho
```

### Land Surface Temperature

With these building blocks, plus the wavelength of the emitted radiance for Landsat 8 $\lambda$, the adjusted LST $\text{T}s$ can be determined using the following formula (converting from Kelvin to Celcius:

$$
\text{T}_s = \frac{\text{BT}}{1 + (\frac{\lambda\text{BT}}{\rho} · ln(\epsilon\lambda))} - 273.15
$$

```{r}
# wavelength
wl = 10.895e-6

# calc and plot
ts = bt/(1+(wl*bt/rho)*log(e))-273.15

plot_rast(ts,
          expression(Land~Surface~Temperature~(T[s])), c("blue", "red"))
```

#### Leaflet Visualisation of LST

Using the Leaflet Package for R [@leaflet], we can visualise the resulting rast

```{r}
plet(ts,
     col=(c("darkblue","pink")),
     legend="bottomright",
     main="LST (Cº)",
     tiles=c("Esri.WorldImagery"))

```

## 

# Model Building

Now that we have gathered the data, the next step is to develop several models to predict Land Surface Temperature based on our environmental inputs.

## General Linear Models

The first GLM model is simply multivariate linear regression fitted within a OLS framework.

### Fitting the Model

The formula for this model is:

$$
\text{T}_{si} = a_0+a _1\text{Tourism}_i+a _2\text{Buildings}_i+a _3\text{NDVI}_i+a _4\text{NDWI}_i+a _5\text{Coast}_i + a _6\text{Elevation}_i + \epsilon_i
$$

Where $i$ denotes the individual 30m raster tiles and $\epsilon$ denotes the residual error not accounted for.

Also, there is analysis of a potential interaction effect between tourism distance and NDBI, as the types of built up area may vary as we move from a tourism heavy area, thus also adding the following term to the OLS function.

$$
a_7\text{Tourism · NDVI}
$$

We will run these models at all levels of building concentration and only at the high concentrations of buildings (\>2)

```{r}
rast_stack = c(ts, ndvi, ndwi, coastdistance, scale(albedo),
               tourism_heat, building_heat, strm)

data_df = na.omit(as.data.frame(rast_stack, xy=TRUE))

names(data_df) = c("X", "Y", 
                   "LST", "NDVI", "NDWI", "CoastDistance",
                   "Albedo", "TourismExposure",
                   "BuildingExposure", "Elevation")

# define high density
hd_data_df = data_df %>% filter(BuildingExposure > 0.5)

# compute models
ols_models = list(
  "OLS 1" = lm(LST ~ 
               TourismExposure +
               BuildingExposure +
               Albedo +
               NDVI +
               NDWI +
               CoastDistance +
               Elevation,
             data=data_df),
  "OLS 2 (Interaction)" = lm(LST ~                                                                                TourismExposure*BuildingExposure +
                             Albedo +
                             NDVI +
                             NDWI +
                             CoastDistance +
                             Elevation,
                           data=data_df),
  "OLS 3 (High Density)" = lm(LST ~ 
                              TourismExposure +
                              BuildingExposure +
                              Albedo +  
                              NDVI +
                              NDWI +
                              CoastDistance +
                              Elevation,
                             data=hd_data_df),
  "OLS 4 (High Density, Interaction)" = lm(LST ~                                                                                TourismExposure*BuildingExposure +
                                           Albedo +    
                                           NDVI +
                                           NDWI +
                                           CoastDistance +
                                           Elevation,
                                          data=hd_data_df))

modelsummary(ols_models, stars = TRUE)
```

### Plot Fit

```{r}
# all building concentrations 
tourism_area = tibble(
  TourismExposure = 2,
  NDVI = mean(data_df$NDVI),
  BuildingExposure = seq(min(data_df$BuildingExposure),
                         max(data_df$BuildingExposure),
                         by = 0.05),
  Albedo = mean(data_df$Albedo),
  NDWI = mean(data_df$NDWI),
  CoastDistance = mean(data_df$CoastDistance),
  Elevation = mean(data_df$Elevation)
)

residence_area = tibble(
  TourismExposure = -0.6,
  NDVI = mean(data_df$NDVI),
  BuildingExposure = seq(min(data_df$BuildingExposure),
                         max(data_df$BuildingExposure),
                         by = 0.05),
  Albedo = mean(data_df$Albedo),
  NDWI = mean(data_df$NDWI),
  CoastDistance = mean(data_df$CoastDistance),
  Elevation = mean(data_df$Elevation)
)

# bind together
scenario = rbind(tourism_area, residence_area)

# predict values
area_predictions = predict(
  ols_models$`OLS 2`,
  newdata = scenario,
  se.fit = TRUE,
  interval = "confidence"
)

# make tibble
area_predictions = area_predictions$fit %>% 
  as_tibble()

# make toplot
toplot = bind_cols(scenario,
                   area_predictions)

toplot = select(toplot, c("fit", "lwr", "upr",
                          "BuildingExposure",
                          "TourismExposure"))

toplot$area = ifelse(toplot$TourismExposure == -0.6, "Residence",
                     "Tourist")

# plot all density

all_density_plot = ggplot() +
  geom_line(data = toplot,
            aes(x = BuildingExposure,
                y = fit,
                colour = area),
            alpha = 2) +
  scale_colour_manual(values = c("Residence" = "blue", "Tourist" = "red"),
                      name = "Area Type") +
  new_scale_color() +
  geom_point(data = data_df,
             aes(x = BuildingExposure,
                 y = LST,
                 colour = TourismExposure),
             alpha = 0.02) +
  scale_color_viridis_c(name = "Tourism Concentration") +
  labs(title = "Effects of Building Concentration on Land Surface Temperature",
       subtitle = "All Building Concentrations,
Maspalomas/Playa del Inglés region of Gran Canaria",
       caption = "Model Based on LandSat Imagery from 27th July 2023",
       x = "Building Concentration Index",
       y = "Predicted Land Surface Temperature ºC")

all_density_plot
```

```{r}
# hd building concentrations 
hd_tourism_area = tibble(
  TourismExposure = 4,
  NDVI = mean(hd_data_df$NDVI),
  BuildingExposure = seq(0.5, 
                        max(hd_data_df$BuildingExposure),
                        by = 0.05),
  Albedo = mean(data_df$Albedo),
  NDWI = mean(hd_data_df$NDWI),
  CoastDistance = mean(hd_data_df$CoastDistance),
  Elevation = mean(hd_data_df$Elevation)
)

hd_residence_area = tibble(
  TourismExposure = -0.6,
  NDVI = mean(hd_data_df$NDVI),
  BuildingExposure = seq(0.5, 
                        max(hd_data_df$BuildingExposure),
                        by = 0.05),
  Albedo = mean(data_df$Albedo),
  NDWI = mean(hd_data_df$NDWI),
  CoastDistance = mean(hd_data_df$CoastDistance),
  Elevation = mean(hd_data_df$Elevation)
)

# bind together
hd_scenario = rbind(hd_tourism_area, hd_residence_area)

# predict values
hd_area_predictions = predict(
  ols_models$`OLS 4`,
  newdata = hd_scenario,
  se.fit = TRUE,
  interval = "confidence"
)

# make tibble
hd_area_predictions = hd_area_predictions$fit %>% 
  as_tibble()

# make toplot
hd_toplot = bind_cols(hd_scenario,
                      hd_area_predictions)
hd_toplot = select(hd_toplot, c("fit", "lwr", "upr",
                                "BuildingExposure",
                                "TourismExposure"))
hd_toplot$area = ifelse(hd_toplot$TourismExposure == -0.6, "Residence",
                        "Tourist")
# plot all density
hd_density_plot = ggplot() +
  geom_line(data = hd_toplot,
            aes(x = BuildingExposure,
                y = fit,
                colour = area),
            alpha = 2) +
  scale_colour_manual(values = c("Residence" = "blue", "Tourist" = "red"),
                      name = "Area Type") +
  new_scale_colour() +
  geom_point(data = hd_data_df,
             aes(x = BuildingExposure,
                 y = LST,
                 colour = TourismExposure),
             alpha = 0.02) +
  scale_colour_viridis_c(name = "Tourism Concentration") +
  labs(title = "Effects of Building Concentration on Land Surface Temperature",
       subtitle = "High Building Concentrations,
Maspalomas/Playa del Inglés region of Gran Canaria",
       caption = "Model Based on LandSat Imagery from 27th July 2023",
       x = "Building Concentration Index",
       y = "Predicted Land Surface Temperature ºC")

hd_density_plot
```

## Single Layer Neural Network

The second model is a straightforward single layer neural network using the nnet package [@nnet]. This is slightly different from the work of @equere2021, who use two hidden layers. The steps for building this model are as follows:

### Normalise the Data

It is best practice to scale all the values in our dataset $X$ to be ranged from -1 to 1. This is achieved with the standard normalisation algorithm.

$$
X_{\text{scaled}} = \frac{x - X_{\text{min}}}{X_{\text{max}} - X_{\text{min}}}
$$

```{r}
# normalize your data
maxs = apply(data_df, 2, max) 
mins = apply(data_df, 2, min)
scaled_data_df = as.data.frame(scale(data_df,
                                     center = mins,
                                     scale = maxs - mins))
```

### Data Splitting

Next it is important to split the data in to train $X_{\text{train}}$ and test $X_{\text{test}}$ sets. One for building the model and one for testing its fit. This split is on an 80:20 ratio respectively.

```{r}
# split the data 
set.seed(1999) # my year of birth
indexes = sample(1:nrow(scaled_data_df),
                 size = 0.8 * nrow(scaled_data_df))
# use index to split
train_data = scaled_data_df[indexes, ]
test_data = scaled_data_df[-indexes, ]
```

### Fitting the Model

To fit the model we first define it as a method to determine LST $T_s$ as a function of the input data $X$ utilsing a set of weights $W$ and biases $b$.

$$
T_s = f(X; W, b)
$$

```{r}
# define the model
formula = LST ~
  NDVI +
  BuildingExposure +
  NDWI +
  CoastDistance +
#  RoadExposure +
  TourismExposure +
  Elevation
```

The model is defined as having one hidden layer and thus two activation steps. The first step is defining inital weights $W^{[1]}$ and biases $b^{[1]}$ to take the input layer to the hidden layer $Z^{[1]}$.

$$
Z^{[1]} = W^{[1]}X + b^{[1]}
$$

This is is then put into an initial activation function $g^{[1]}$ in the hidden layer to get the first activation $A^{[1]}$ $$
A^{[1]} = g^{[1]}(Z^{[1]})
$$

Next this hidden layer activation has applied to it another set of defined weights $W^{[2]}$ and biases $b^{[2]}$ to reach our predicted output $\hat{T_s}$

$$
\hat{T_s} = W^{[2]}A^{[1]} + b^{[2]} 
$$

```{r}
# model has 4 neurons on hidden layer, decay of 0.00004 to prevent overfitting, and 200 iterations
model = nnet(formula,
             data=train_data,
             size=4,
             linout=TRUE, 
             decay=5e-4, 
             maxit=250)
```

### Testing Fit and Predictive Ability

Using the $X_\text{test}$ dataset we can also see the fit of the model and test its predictive ability.

```{r}
# make predictions and test with actuals
predictions = predict(model,
                      newdata=test_data[, -which(names(test_data) == "LST")])

actuals = test_data$LST # actual values
mse = mean((predictions - actuals)^2)
rmse = sqrt(mse)

cat("MSE:", mse, "\n")
cat("RMSE:", rmse, "\n")

# calc r squared
SST = sum((actuals - mean(actuals))^2)
SSR = sum((predictions - actuals)^2)
r_squared = 1 - (SSR/SST)
cat("R^2:", r_squared, "\n")
```

### Plot Fit

We can also plot the fit of the model as a bivariate line graph between predictions and actuals, reversing the normalising first.

```{r}
# reverse norm
max_LST = max(data_df$LST)
min_LST = min(data_df$LST)

predictions_original = predictions * (max_LST - min_LST) + min_LST
actuals_original = actuals * (max_LST - min_LST) + min_LST

# plot
plot(actuals_original, predictions_original, xlab = "Actual", ylab = "Predicted",
     main = "Predicted vs. Actual Values")
abline(0, 1)
```
